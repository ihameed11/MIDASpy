{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MIDASpy demonstration"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides a brief demonstration of **MIDASpy**'s core functionalities. We show how to use the package to impute missing values in the [Adult census dataset](https://github.com/MIDASverse/MIDASpy/blob/master/Examples/adult_data.csv) (which is commonly used for benchmarking machine learning tasks).\n",
        "\n",
        "Users of **MIDASpy** must have **TensorFlow** installed as a **pip** package in their Python environment. **MIDASpy** is compatible with both **TensorFlow** 1.X and **TensorFlow** >= 2.2 versions.\n",
        "\n\nOnce these packages are installed, users can import the dependencies and load the data:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import MIDASpy as md\n",
        "\n",
        "data_0 = pd.read_csv('adult_data.csv')\n",
        "data_0.columns.str.strip()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'age', 'workclass', 'fnlwgt', 'education',\n",
              "       'education_num', 'marital_status', 'occupation', 'relationship', 'race',\n",
              "       'sex', 'capital_gain', 'capital_loss', 'hours_per_week',\n",
              "       'native_country', 'class_labels'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the Adult dataset has very little missingness, we randomly set 5,000 observed values as missing in each column:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(441)\n",
        "\n",
        "def spike_in_generation(data):\n",
        "    spike_in = pd.DataFrame(np.zeros_like(data), columns= data.columns)\n",
        "    for column in data.columns:\n",
        "        subset = np.random.choice(data[column].index[data[column].notnull()], 5000, replace= False)\n",
        "        spike_in.loc[subset, column] = 1\n",
        "    return spike_in\n",
        "\n",
        "spike_in = spike_in_generation(data_0)\n",
        "original_value = data_0.loc[4, 'hours_per_week']\n",
        "data_0[spike_in == 1] = np.nan"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we list categorical variables in a vector and one-hot encode them using **MIDASpy**'s inbuilt preprocessing function `cat_conv`, which returns both the encoded data and a nested list of categorical column names we can pass to the imputation algorithm. To construct the final, pre-processed data we append the one-hot encoded categorical data to the non-cateogrical data, and replace null values with `np.nan` values:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "categorical = ['workclass','marital_status','relationship','race','class_labels','sex','education','occupation','native_country']\n",
        "data_cat, cat_cols_list = md.cat_conv(data_0[categorical])\n",
        "\n",
        "data_0.drop(categorical, axis = 1, inplace = True)\n",
        "constructor_list = [data_0]\n",
        "constructor_list.append(data_cat)\n",
        "data_in = pd.concat(constructor_list, axis=1)\n",
        "\n",
        "na_loc = data_in.isnull()\n",
        "data_in[na_loc] = np.nan"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize the results:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_in.head())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0   age    fnlwgt  education_num  capital_gain  capital_loss  \\\n",
            "0         0.0  39.0   77516.0           13.0        2174.0           0.0   \n",
            "1         1.0  50.0   83311.0           13.0           0.0           0.0   \n",
            "2         2.0  38.0  215646.0            9.0           0.0           NaN   \n",
            "3         3.0  53.0  234721.0            NaN           NaN           0.0   \n",
            "4         NaN  28.0       NaN           13.0           0.0           NaN   \n",
            "\n",
            "   hours_per_week  workclass_Federal-gov  workclass_Local-gov  \\\n",
            "0            40.0                    0.0                  0.0   \n",
            "1            13.0                    0.0                  0.0   \n",
            "2            40.0                    NaN                  NaN   \n",
            "3            40.0                    0.0                  0.0   \n",
            "4             NaN                    0.0                  0.0   \n",
            "\n",
            "   workclass_Never-worked  ...  native_country_Portugal  \\\n",
            "0                     0.0  ...                      0.0   \n",
            "1                     0.0  ...                      0.0   \n",
            "2                     NaN  ...                      0.0   \n",
            "3                     0.0  ...                      0.0   \n",
            "4                     0.0  ...                      0.0   \n",
            "\n",
            "   native_country_Puerto-Rico  native_country_Scotland  native_country_South  \\\n",
            "0                         0.0                      0.0                   0.0   \n",
            "1                         0.0                      0.0                   0.0   \n",
            "2                         0.0                      0.0                   0.0   \n",
            "3                         0.0                      0.0                   0.0   \n",
            "4                         0.0                      0.0                   0.0   \n",
            "\n",
            "   native_country_Taiwan  native_country_Thailand  \\\n",
            "0                    0.0                      0.0   \n",
            "1                    0.0                      0.0   \n",
            "2                    0.0                      0.0   \n",
            "3                    0.0                      0.0   \n",
            "4                    0.0                      0.0   \n",
            "\n",
            "   native_country_Trinadad&Tobago  native_country_United-States  \\\n",
            "0                             0.0                           1.0   \n",
            "1                             0.0                           1.0   \n",
            "2                             0.0                           1.0   \n",
            "3                             0.0                           1.0   \n",
            "4                             0.0                           0.0   \n",
            "\n",
            "   native_country_Vietnam  native_country_Yugoslavia  \n",
            "0                     0.0                        0.0  \n",
            "1                     0.0                        0.0  \n",
            "2                     0.0                        0.0  \n",
            "3                     0.0                        0.0  \n",
            "4                     0.0                        0.0  \n",
            "\n",
            "[5 rows x 108 columns]\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data are now ready to be fed into the imputation algorithm, which involves three steps. First, we specify the dimensions, input corruption proportion, and other hyperparameters of the MIDAS neural network. Second, we build a MIDAS model based on the data. The vector of one-hot-encoded column names should be passed to the softmax_columns argument, as MIDAS employs a softmax final-layer activation function for categorical variables. Third, we train the model on the data, setting the number of training epochs as 20 in this example:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = md.Midas(layer_structure = [256,256], vae_layer = False, seed = 89, input_drop = 0.75)\n",
        "imputer.build_model(data_in, softmax_columns = cat_cols_list)\n",
        "imputer.train_model(training_epochs = 20)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size index: [7, 8, 7, 6, 5, 2, 2, 16, 14, 41]\n",
            "\n",
            "Computation graph constructed\n",
            "\n",
            "Model initialised\n",
            "\n",
            "Epoch: 0 , loss: 117256.53483883518\n",
            "Epoch: 1 , loss: 86191.57900557012\n",
            "Epoch: 2 , loss: 82235.14160604215\n",
            "Epoch: 3 , loss: 80032.84553225857\n",
            "Epoch: 4 , loss: 77202.56478397874\n",
            "Epoch: 5 , loss: 72421.6698374017\n",
            "Epoch: 6 , loss: 68203.67437592153\n",
            "Epoch: 7 , loss: 67226.95568095717\n",
            "Epoch: 8 , loss: 66277.40566796619\n",
            "Epoch: 9 , loss: 65670.3376109672\n",
            "Epoch: 10 , loss: 65726.79206258191\n",
            "Epoch: 11 , loss: 66133.58962712719\n",
            "Epoch: 12 , loss: 65309.62404327592\n",
            "Epoch: 13 , loss: 65309.563779258475\n",
            "Epoch: 14 , loss: 64963.398904342954\n",
            "Epoch: 15 , loss: 65626.56983061825\n",
            "Epoch: 16 , loss: 64806.52387842501\n",
            "Epoch: 17 , loss: 65535.00585041571\n",
            "Epoch: 18 , loss: 65120.4375428766\n",
            "Epoch: 19 , loss: 65258.038586377785\n",
            "Training complete. Saving file...\n",
            "Model saved in file: tmp/MIDAS\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": [
              "<MIDASpy.midas_base.Midas at 0x7fdc29165700>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once training is complete, we can generate any number of imputed datasets (M) using the `generate_samples` function (here we set M as 10). Users can then either write these imputations to separate .CSV files or work with them directly in Python:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "imputations = imputer.generate_samples(m=10).output_list \n",
        "\n",
        "# for i in imputations:\n",
        "#    file_out = ``midas_imp_\" + str(n) + ``.csv\"\n",
        "#    i.to_csv(file_out, index=False)\n",
        "#    n += 1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
            "Model restored.\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, using the list of generated imputations, we can estimate M separate regression models and combine the parameter and variance estimates (see Rubin 1987) using **MIDASpy's** `combine` function:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = md.combine(y_var = \"capital_gain\", \n",
        "                   X_vars = [\"education_num\",\"age\"],\n",
        "                   df_list = imputations)\n",
        "\nmodel"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 36,
          "data": {
            "text/plain": [
              "            term   estimate   std.error  statistic          df   p.value\n",
              "0          const -98.103910  100.060298  -0.980448  162.300742  0.328324\n",
              "1  education_num  18.190052    3.836454   4.741371  403.771392  0.000003\n",
              "2            age  20.059125    2.099022   9.556413  151.303420  0.000000"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>term</th>\n",
              "      <th>estimate</th>\n",
              "      <th>std.error</th>\n",
              "      <th>statistic</th>\n",
              "      <th>df</th>\n",
              "      <th>p.value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>const</td>\n",
              "      <td>-98.103910</td>\n",
              "      <td>100.060298</td>\n",
              "      <td>-0.980448</td>\n",
              "      <td>162.300742</td>\n",
              "      <td>0.328324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>education_num</td>\n",
              "      <td>18.190052</td>\n",
              "      <td>3.836454</td>\n",
              "      <td>4.741371</td>\n",
              "      <td>403.771392</td>\n",
              "      <td>0.000003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>age</td>\n",
              "      <td>20.059125</td>\n",
              "      <td>2.099022</td>\n",
              "      <td>9.556413</td>\n",
              "      <td>151.303420</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 36,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
